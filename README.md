# Securing large language Models: A comprehensive study of attacks and countermeasures.
**Research Project**

## Status: **In Progress** 


## ğŸ“š Overview

This is my ongoing MSc Information Security research project at Royal Holloway, University of London. It focuses on identifying, analysing, and mitigating emerging security threats targeting **Large Language Models (LLMs)** such as GPT-4, Claude, LLaMA, and Falcon.

As LLMs become integral to real-world systems, attackers are developing techniques to exploit vulnerabilities like prompt injection, data leakage, adversarial attacks, and model poisoning. My research combines practical testing with academic analysis to propose scalable, real-world countermeasures.

---

## ğŸ¯ Key Research Questions

- What are the most critical security threats to LLMs today?
- How do current defences perform under real-world adversarial attacks?
- What strategies are most effective at securing LLMs without breaking functionality?

---

## ğŸ› ï¸ Methodology

1. **Literature Review**  
   Review 10+ top academic and industry papers on LLM vulnerabilities, incidents, and defences.

2. **Testing LLM Vulnerabilities**  
   Perform prompt injection, model inversion, adversarial perturbation, and poisoning attacks on:
   - Open-source models: LLaMA, Falcon, Mistral
   - Proprietary models: GPT-4, Claude, Gemini

3. **Build an Attack Simulation Framework**  
   Python-based modular tool to automate and evaluate LLM security attacks and defences.

4. **Implement & Compare Countermeasures**  
   Test defences like:
   - Prompt filtering
   - Adversarial training
   - Differential privacy
   - Model guardrails

5. **Analysis & Recommendations**  
   - Measure attack success rates and mitigation performance
   - Propose best practices for securing LLMs in production

---

## ğŸ’¡ Expected Deliverables

- ğŸ“„ A detailed research paper (Summer 2025)
- âš™ï¸ An open-source attack simulation framework
- ğŸ“Š Security evaluation benchmarks and recommendations

---

## ğŸ” Status

> **ğŸ§ª Currently in active research & development.**

---

## ğŸŒ Why It Matters

LLMs are transforming business, healthcare, security, and more â€” but without effective safeguards, they introduce new risks. This research bridges the gap between AI innovation and cybersecurity resilience, making it relevant for:
- AI Security Engineers
- SOC Teams
- Red/Blue Teams
- AI Product Developers
- Cybersecurity Researchers

---

ğŸ“¬ Contact: Gokul.Parandhaman.2024@live.rhul.ac.uk  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/gokul-parandhaman-263762283)
